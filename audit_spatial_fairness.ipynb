{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing for Spatial Fairness\n",
    "\n",
    "This notebook runs the experiments. The methods are implemented in the `src/function.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "                \n",
    "from functions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Select the dataset\n",
    "\n",
    "You can select one of the following:\n",
    "- LAR (`data/LAR.csv`) contains the modified Loan/Application Register records in the US for Bank of America for the year 2021; the dataset is created by `data/LAR/create_LAR.ipynb`.\n",
    "- Crime (`data/Crime.csv`) contains predictions about crime incidents in the city of Los Angeles from 2010â€“2019; the predictive model is a Random Forest Classifier and the dataset is created by `data/Crime/create_Crime.ipynb`.\n",
    "- Synth_fair/Synth_unfair/Semisynth (`/data/Synth_fair.csv`, `/data/Synth_unfair.csv`, `/data/Semisynth.csv`) are synthetic datasets create by the notebooks in `data/Synth/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=206418 points\n",
      "P=127286 positives\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_taken</th>\n",
       "      <th>census_tract</th>\n",
       "      <th>location</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10003015200</td>\n",
       "      <td>(39.7070504, -75.5832416)</td>\n",
       "      <td>39.707050</td>\n",
       "      <td>-75.583242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6059086502</td>\n",
       "      <td>(33.8503559, -117.9121351)</td>\n",
       "      <td>33.850356</td>\n",
       "      <td>-117.912135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>26163596100</td>\n",
       "      <td>(42.1014482, -83.1602786)</td>\n",
       "      <td>42.101448</td>\n",
       "      <td>-83.160279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>9009165600</td>\n",
       "      <td>(41.3558996, -72.9323558)</td>\n",
       "      <td>41.355900</td>\n",
       "      <td>-72.932356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>36061001200</td>\n",
       "      <td>(40.7159065, -73.9820936)</td>\n",
       "      <td>40.715907</td>\n",
       "      <td>-73.982094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_taken  census_tract                    location        lat  \\\n",
       "0             1   10003015200   (39.7070504, -75.5832416)  39.707050   \n",
       "1             1    6059086502  (33.8503559, -117.9121351)  33.850356   \n",
       "2             1   26163596100   (42.1014482, -83.1602786)  42.101448   \n",
       "3             1    9009165600   (41.3558996, -72.9323558)  41.355900   \n",
       "4             3   36061001200   (40.7159065, -73.9820936)  40.715907   \n",
       "\n",
       "          lon  \n",
       "0  -75.583242  \n",
       "1 -117.912135  \n",
       "2  -83.160279  \n",
       "3  -72.932356  \n",
       "4  -73.982094  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load the LAR dataset\n",
    "df = load_data('./data/LAR.csv')\n",
    "label = 'action_taken'\n",
    "## df = filterbbox(df, -87.634938, 24.523096, -80.031362, 31.000888) ## florida\n",
    "## df = filterbbox(df, -80.8736, 25.13742, -80.06279, 25.979434) # miami\n",
    "\n",
    "\n",
    "## load the CRIME_serious dataset\n",
    "# df = load_data('./data/Crime.csv')\n",
    "# label = 'pred'\n",
    "\n",
    "\n",
    "# load a synthetic dataset\n",
    "# df = load_data('./data/Synth_fair.csv') ## FAIR\n",
    "# label = 'label'\n",
    "\n",
    "\n",
    "# df = load_data('./data/Synth_unfair.csv') ## UNFAIR\n",
    "# label = 'label'\n",
    "\n",
    "\n",
    "# df = load_data('./data/Semisynth.csv') ## FAIR\n",
    "# label = 'label'\n",
    "\n",
    "\n",
    "N, P = get_stats(df, label)\n",
    "\n",
    "print(f'N={N} points')\n",
    "print(f'P={P} positives')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_types = get_true_types(df, label)\n",
    "# print(true_types[:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtree = create_rtree(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat_max = df['lat'].values.max()\n",
    "# lat_min = df['lat'].values.min()\n",
    "# lon_max = df['lon'].values.max()\n",
    "# lon_min = df['lon'].values.min()\n",
    "\n",
    "# mapit = folium.Map(location=[37.09, -95.71], zoom_start=5, tiles=\"Stamen Toner\")\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if row[label] == 1:\n",
    "#         folium.CircleMarker( location=(row['lat'], row['lon']), color='#00FF00', fill_color='#00FF00', fill=True, opacity=0.4, fill_opacity=0.4, radius=2 ).add_to( mapit )\n",
    "#     elif row[label] == 0:\n",
    "#         folium.CircleMarker( location=(row['lat'], row['lon']), color='#FF0000', fill_color='#FF0000', fill=True, opacity=0.4, fill_opacity=0.4, radius=2 ).add_to( mapit )\n",
    "\n",
    "\n",
    "# mapit.fit_bounds([(lat_min, lon_min), (lat_max, lon_max)])\n",
    "\n",
    "# mapit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Run Experiments\n",
    "\n",
    "There are three experiments:\n",
    "- Unrestricted regions: runs **our approach** on unrestricted regions.\n",
    "- One Partitioning: runs **our approach** against **MeanVar** on regions from a single partitioning.\n",
    "- Multiple Partitionings: runs **MeanVar** on multiple partitionings.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unrestricted regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = create_seeds(df, rtree, 100)\n",
    "# print(len(seeds), seeds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 regions\n"
     ]
    }
   ],
   "source": [
    "radii = np.arange(0.05, 1.01, 0.05)\n",
    "regions = create_regions(df, rtree, seeds, radii)\n",
    "\n",
    "print(len(regions), 'regions')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapit = folium.Map(location=[37.09, -95.71], zoom_start=5, tiles=\"Stamen Toner\")\n",
    "\n",
    "for point in seeds:\n",
    "    folium.CircleMarker( location=id2loc(df, point), color='#0000FF', fill_color='#0000FF', fill=True, opacity=0.4, fill_opacity=0.4, radius=2 ).add_to( mapit )\n",
    "\n",
    "center = (26, -126)\n",
    "\n",
    "r = radii[0]\n",
    "folium.Rectangle([(center-r, center-r), (center+r, center+r)], color='#F1CF3B').add_to( mapit )\n",
    "r = radii[-1]\n",
    "folium.Rectangle([(center-r, center-r), (center+r, center+r)], color='#F1CF3B').add_to( mapit )\n",
    "\n",
    "\n",
    "mapit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range 4.656030796468258e-07 1963.0672387208906\n",
      "max likelihood 1963.0672387208906\n",
      "n=17874, p=14719\n",
      "rho=0.6166419595190342, rho_in=0.8234866286225803, rho_out=0.5970330532926001\n",
      "l0max=-137409.18985450186, l1max=-135446.12261578097, statistic=1963.0672387208906\n"
     ]
    }
   ],
   "source": [
    "direction = 'both'\n",
    "# direction = 'less_in'\n",
    "# direction = 'less_out'\n",
    "\n",
    "best_region, max_likeli, statistics = scan_regions(regions, true_types, N, P, direction=direction, verbose=True)\n",
    "\n",
    "# statistics.sort(key=lambda x: -x)\n",
    "# print(statistics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.920751262805425\n"
     ]
    }
   ],
   "source": [
    "## determine the significance threshold based on a desired signif_level\n",
    "\n",
    "n_alt_worlds = 200\n",
    "signif_level = 0.005\n",
    "\n",
    "signif_thresh = get_signif_threshold(signif_level, n_alt_worlds, regions, N, P)\n",
    "print(signif_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695 significant regions\n"
     ]
    }
   ],
   "source": [
    "## identify regions with statistic above statistical significance threshold\n",
    "\n",
    "sorted_statistics = np.sort(statistics)\n",
    "\n",
    "top_k = len(statistics) - np.searchsorted(sorted_statistics, signif_thresh)\n",
    "\n",
    "print(top_k, 'significant regions')\n",
    "\n",
    "\n",
    "indexes = np.argsort(statistics)[::-1][:top_k]\n",
    "\n",
    "significant_regions = [ regions[i] for i in indexes ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 non-overlapping regions\n"
     ]
    }
   ],
   "source": [
    "def intersects(regionA, regionB):\n",
    "    cA = np.array(id2loc(df, regionA['center']))\n",
    "    cB = np.array(id2loc(df, regionB['center']))\n",
    "    rA = regionA['radius']\n",
    "    rB = regionB['radius']\n",
    "\n",
    "    A_top_right = cA + np.array([rA, rA])\n",
    "    A_bottom_left = cA - np.array([rA, rA])\n",
    "    B_top_right = cB + np.array([rB, rB])\n",
    "    B_bottom_left = cB - np.array([rB, rB])\n",
    "\n",
    "    # print(A_bottom_left, A_top_right, B_bottom_left, B_top_right)\n",
    "\n",
    "    return not (A_top_right[0] < B_bottom_left[0] or A_bottom_left[0] > B_top_right[0] or A_top_right[1] < B_bottom_left[1] or A_bottom_left[1] > B_top_right[1])\n",
    "\n",
    "\n",
    "\n",
    "non_olap_regions = []\n",
    "centers = []\n",
    "for region in significant_regions:\n",
    "    center = region['center']\n",
    "    if center in centers:\n",
    "        continue\n",
    "    \n",
    "    no_intersections = True\n",
    "    for other in non_olap_regions:\n",
    "        if intersects(region, other):\n",
    "            no_intersections = False\n",
    "            break\n",
    "    if no_intersections:\n",
    "        centers.append(center)\n",
    "        non_olap_regions.append(region)\n",
    "    # print(region['radius'])\n",
    "\n",
    "print(len(non_olap_regions), 'non-overlapping regions')\n",
    "\n",
    "# over(non_olap_regions[0], non_olap_regions[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600 7028\n"
     ]
    }
   ],
   "source": [
    "## find smallest, largest regions\n",
    "\n",
    "min_radius = np.inf\n",
    "max_radius = -np.inf\n",
    "for region in non_olap_regions:\n",
    "    if region['radius'] < min_radius:\n",
    "        min_radius = region['radius']\n",
    "        # region_min_radius = region\n",
    "    if region['radius'] > max_radius:\n",
    "        max_radius = region['radius']\n",
    "        # region_max_radius = region\n",
    "\n",
    "min_points = np.inf\n",
    "max_points = -np.inf\n",
    "for region in non_olap_regions:\n",
    "    if region['radius'] == min_radius and len(region['points']) < min_points:\n",
    "        min_points = len(region['points'])\n",
    "        region_min_radius = region\n",
    "    if region['radius'] == max_radius and len(region['points']) > max_points:\n",
    "        max_points = len(region['points'])\n",
    "        region_max_radius = region\n",
    "\n",
    "print(len(region_min_radius['points']), len(region_max_radius['points']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_circular_regions(df, true_types, non_olap_regions[:5])\n",
    "\n",
    "# show_circular_regions(df, true_types, [region_min_radius, region_max_radius])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Partitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partitioning(df, rtree, lon_min: float, lon_max: float, lat_min: float, lat_max: float, lon_n: float, lat_n: float):\n",
    "    grid_info = {}\n",
    "    grid_info['lon_min'] = lon_min\n",
    "    grid_info['lon_max'] = lon_max\n",
    "    grid_info['lat_min'] = lat_min\n",
    "    grid_info['lat_max'] = lat_max\n",
    "    grid_info['lat_n'] = lat_n\n",
    "    grid_info['lon_n'] = lon_n\n",
    "\n",
    "    grid_loc2_idx = {} ## maps (x,y) grid_loc coords to an index in the partitions array\n",
    "\n",
    "    partitions = []\n",
    "    for i in range(lat_n):\n",
    "        lat_start = lat_min + (i/lat_n)*(lat_max - lat_min)\n",
    "        lat_end = lat_min + ((i+1)/lat_n)*(lat_max - lat_min)\n",
    "        for j in range(lon_n):\n",
    "            lon_start = lon_min + (j/lon_n)*(lon_max - lon_min)\n",
    "            lon_end = lon_min + ((j+1)/lon_n)*(lon_max - lon_min)\n",
    "\n",
    "            points = query_range_box(df, rtree, lon_start, lon_end, lat_start, lat_end)\n",
    "            # print(len(points))\n",
    "            partition  = {\n",
    "                'grid_loc': (j, i),\n",
    "                'points' : points,\n",
    "            }\n",
    "            grid_loc2_idx[(j,i)] = len(partitions)\n",
    "            partitions.append(partition)\n",
    "    \n",
    "    return grid_info, grid_loc2_idx, partitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.5361762 64.8944045 -161.8508035 -67.1081422\n"
     ]
    }
   ],
   "source": [
    "lat_max = df['lat'].values.max()\n",
    "lat_min = df['lat'].values.min()\n",
    "lon_max = df['lon'].values.max()\n",
    "lon_min = df['lon'].values.min()\n",
    "print(lat_min, lat_max, lon_min, lon_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create the partitioning (grid) and its partitions (regions)\n",
    "\n",
    "# lat_n = 12 ## number of partitions along vertical axis (latitude)  ## was 12\n",
    "# lon_n = 25 ## number of partitions along horizontal axis (longitude) ## was 25\n",
    "\n",
    "lat_n = 20\n",
    "lon_n = 20\n",
    "\n",
    "\n",
    "grid_info, grid_loc2_idx, regions = create_partitioning(df, rtree, lon_min, lon_max, lat_min, lat_max, lon_n, lat_n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range 0.0 1313.1253486430214\n",
      "max likelihood 1313.1253486430214\n",
      "n=13656, p=11101\n",
      "rho=0.6166419595190342, rho_in=0.8129027533684827, rho_out=0.6027380915325635\n",
      "l0max=-137409.18985450186, l1max=-136096.06450585884, statistic=1313.1253486430214\n"
     ]
    }
   ],
   "source": [
    "best_region, max_likeli, statistics = scan_regions(regions, true_types, N, P, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.900119686091784\n"
     ]
    }
   ],
   "source": [
    "## determine the significance threshold based on a desired signif_level\n",
    "\n",
    "n_alt_worlds = 1000\n",
    "signif_level = 0.005\n",
    "\n",
    "signif_thresh = get_signif_threshold(signif_level, n_alt_worlds, regions, N, P)\n",
    "print(signif_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 significant regions\n"
     ]
    }
   ],
   "source": [
    "## identify regions with statistic above statistical significance threshold\n",
    "\n",
    "sorted_statistics = np.sort(statistics)\n",
    "# print(sorted_statistics[::-1][40:60])\n",
    "# print(np.sort(statistics)[::-1][40:60])\n",
    "\n",
    "top_k = len(statistics) - np.searchsorted(sorted_statistics, signif_thresh)\n",
    "\n",
    "print(top_k, 'significant regions')\n",
    "\n",
    "\n",
    "indexes = np.argsort(statistics)[::-1][:top_k]\n",
    "\n",
    "significant_regions = [ regions[i] for i in indexes ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_grid_region(df, grid_info, true_types, best_region)\n",
    "show_grid_regions(df, grid_info, true_types, significant_regions[:])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeanVar Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_score 0.3064977230247631 with 8 points\n",
      "[0.3064977230247631 0.3064977230247631 0.3064977230247631\n",
      " 0.3064977230247631 0.3064977230247631]\n"
     ]
    }
   ],
   "source": [
    "## partioning-based scan\n",
    "\n",
    "the_region, max_score, scores = scan_partitioning(regions, true_types)\n",
    "\n",
    "print('max_score', max_score, 'with', len(the_region['points']), 'points')\n",
    "\n",
    "\n",
    "## get the top_k regions\n",
    "\n",
    "top_k = 5\n",
    "\n",
    "ma = np.ma.masked_array(scores, mask=np.isnan(scores))\n",
    "\n",
    "print(-np.sort(-ma)[:top_k])\n",
    "\n",
    "indexes = np.argsort(-ma)[:top_k]\n",
    "\n",
    "# print(indexes)\n",
    "\n",
    "top_regions = [ regions[i] for i in indexes ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_grid_region(df, grid_info, true_types, the_region)\n",
    "show_grid_regions(df, grid_info, true_types, top_regions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 7) (7, 12)\n",
      "148 247\n",
      "1313.1253486430214 0.9587897912424523\n",
      "0.06722631979411768 0.3064977230247631\n"
     ]
    }
   ],
   "source": [
    "## best_region vs the_region\n",
    "\n",
    "the_region = top_regions[0]\n",
    "\n",
    "best_idx = grid_loc2_idx[best_region['grid_loc']]\n",
    "the_idx = grid_loc2_idx[the_region['grid_loc']]\n",
    "\n",
    "print(best_region['grid_loc'], the_region['grid_loc'])\n",
    "print(best_idx, the_idx)\n",
    "\n",
    "print(statistics[best_idx], statistics[the_idx])\n",
    "print(scores[best_idx], scores[the_idx])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Partitionings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.5361762 64.8944045 -161.8508035 -67.1081422\n"
     ]
    }
   ],
   "source": [
    "lat_max = df['lat'].values.max()\n",
    "lat_min = df['lat'].values.min()\n",
    "lon_max = df['lon'].values.max()\n",
    "lon_min = df['lon'].values.min()\n",
    "print(lat_min, lat_max, lon_min, lon_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_n_range = (10, 40)\n",
    "lon_n_range = (10, 40)\n",
    "\n",
    "n_partitionings = 100\n",
    "\n",
    "partitionings = []\n",
    "\n",
    "for i in range(n_partitionings):\n",
    "    lat_n = random.randint(*lat_n_range)\n",
    "    lon_n = random.randint(*lon_n_range)\n",
    "\n",
    "    grid_info, grid_loc2_idx, regions = create_partitioning(df, rtree, lon_min, lon_max, lat_min, lat_max, lon_n, lat_n)\n",
    "\n",
    "    partitionings.append((grid_info, grid_loc2_idx, regions))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeanVar Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of means=0.0376\n",
      "max of maxs=0.3351\n"
     ]
    }
   ],
   "source": [
    "mean_scores = []\n",
    "max_scores = []\n",
    "for partitioning in partitionings:\n",
    "    the_region, max_score, scores = scan_partitioning(partitioning[2], true_types)\n",
    "    mean_score = np.nanmean(scores)\n",
    "    mean_scores.append(mean_score)\n",
    "    \n",
    "    max_scores.append(max_score)\n",
    "\n",
    "    # print(f'{mean_score=:.4f}, {max_score=:.4f}')\n",
    "\n",
    "print(f'mean of means={np.mean(mean_scores):.4f}')\n",
    "print(f'max of maxs={np.max(max_scores):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b6166e26f2a0904529a2c96bcb3fef7fe9e6ff718962e2574762ea40b0420ba2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
